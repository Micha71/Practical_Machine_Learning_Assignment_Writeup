# Prediction Assignment Writeup

author: Micha Bouts  
date: February 21, 2016  


## Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify **how well** they do it. In this project, we use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the **quality** of the physical exercise. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways, defined as the *classe* outcome. 

Both a training and test set were made available. While exploring the dataset we reduced the number of variables from 160 to merely 45 on which we trained a couple of models. The *Random Forests Model* turned out giving the highest accuracy with an out of sample error of 0.8%. This performance was sufficiently good for use as a **prediction** tool in assessing the **quality** of the Unilateral Dumbbell Biceps Curl on 20 test cases.


## Initialization

As a starter we set the working directory and load a number of libraries which we'll use during the data analysis.

```{r, initialization_chunk, warning = FALSE, message = FALSE, echo = TRUE}

setwd("~/R/Coursera/Practical_Machine_Learning"); rm(list = ls())
library(knitr);library(dplyr);library(corrplot);library(rattle); library(randomForest)
library(rpart);library(caret);library(parallel);library(doParallel)

```


## Getting Data from Open Source

A Weight Lifting Exercise Dataset is public available at http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). Both a training and a test set are provided.

Courtesy to 

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

After downloading the datasets we load them in the R environment.

```{r, getting_data_chunk, echo = TRUE}

if (!file.exists("data")) {

        dir.create("data")

        fileUrlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
        fileUrlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

        download.file(fileUrlTrain, destfile = "./data/pml-training.csv")
        download.file(fileUrlTest, destfile = "./data/pml-testing.csv")
        
        }

list.files("./data")

training <- read.csv("./data/pml-training.csv", na.strings = c("NA", "")) 
testing <- read.csv("./data/pml-testing.csv", na.strings = c("NA", ""))

```


## Exploratory Data Analysis

```{r, understanding_data_chunk, fig.width = 20, fig.height = 20, echo = TRUE}

dim(training); dim(testing)
str(training, list.len = 10)

table(training$classe)

```

The training and test datasets contain `r dim(training)[1]` and `r dim(testing)[1]` observations respectively. Both datasets have `r dim(training)[2]` variables. The outcome variable of interest *classe* is a qualitative index of how well the Unilateral Dumbbell Biceps Curl is performed. The table above shows the number of training data per type of *classe*.

- Class A is according to the specification
- Class B is throwing the elbows to the front
- Class C is lifting the dumbbell only halfway
- Class D is lowering the dumbbell only halfway
- Class E is throwing the hips to the front

The intention of this data analysis is to predict the *classe* for each observation in the given test set. We don't look at the test set for now. We get back to that at the very end after having selected the prediction model. 

The first 7 variables *X*, *user_name*, *raw_timestamp_part_1*, *raw_timestamp_part_2*, *cvtd_timestamp*, *new_window* and *num_window* represent general background information. It might be there is a time related effect in our data. To avoid unnecessary complexity we remove these qualifiers for now from our scope of interest.

As pointed out below 100 out of 160 variables have large quantities of missing values (NA's). Removing these variables dramatically reduces the dataset complexity without missing precious information. 

```{r, removing_general_info_variables_and_NA_chunk, echo = TRUE}

table(colSums(!is.na(training)))

dftraining1 <- select(training, -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
dftraining1 <- select(dftraining1, which(colSums(!is.na(dftraining1)) == 19622))

dim(dftraining1)

```

The downsized dataset has only `r dim(dftraining1)[2]` variables. Furthermore, predictor variables which are strongly correlated with each other don't give extra information, yet do pile up the actual error. Hence we better filter them out. A correlation diagram is meaningful for identifying highy correlated regressors. We cluster them by correlation index. Dark blue and dark red bullets on the diagram below represent strongly related regressors which we remove at a cutoff correlation value of 85% or higher. This level is arbitrary chosen.

```{r, correlation_diagram_chunk, fig.width=20, fig.height=20, echo = TRUE}

trainCor1 <- cor(dftraining1[-53])
corrplot(trainCor1, order = "hclust", tl.cex = 1.5, mar = c(5, 4, 2, 2), title = "Correlation Diagram")
```
                                         
```{r, removing_highly_correlated_regressors, echo = TRUE}

highlyCorTraining85 <- findCorrelation(trainCor1, cutoff = 0.85, names = FALSE)
dftraining2 <- dftraining1[ ,-highlyCorTraining85]

dim(dftraining2)

```

As such we reach a final dataset with `r dim(dftraining2)[2]` variables. Before we start model building let's first split this dataset in a smaller training set and a validation set. We sacrifice part of the initial training data in order to create validation data on which we assess the out of sample error rate.

```{r, data_splitting_chunk, echo = TRUE}

inTrain <- createDataPartition(y = dftraining2$classe, p = 0.75, list = FALSE)
trainingSet <- dftraining2[ inTrain, ]
validationSet <- dftraining2[-inTrain, ]

dim(trainingSet); dim(validationSet)

```

## Model Building

Because the outcome *classe* is a categorical variable we'll examine models which can deal with this type of outcome. Therefore we select algorithms based on a *Tree* and a *Random Forests Model*. 

### Tree Model

This classification tree finds the subsequent variables that best separates the outcome. This continues until the group is small enough or the classification is reasonable pure, as such mimimizing classification errors.

```{r, tree_model_chunk, cache = TRUE, echo = TRUE}

set.seed(333)

modFitTree <- train(classe ~ ., method = "rpart", data = trainingSet)
print(modFitTree$finalModel)

fancyRpartPlot(modFitTree$finalModel, sub = "Classification Tree with 6 Final Node Classe Outcomes")

```


### Random Forests Model

We make use of multi-core parallel processing to circumvent the limitation of a 6GB RAM memory at use. Furthermore we control the *train* function by selection a 5-fold *cv* cross validation. Also this will keep the processor time in check.

```{r, random_forest_chunk, warning = FALSE, message = FALSE, cache = TRUE, echo = TRUE}

# configure parallel processing

cluster <- makeCluster(detectCores() -1)
registerDoParallel(cluster)

set.seed(123)

modFitRFcv <- train(classe ~ ., method = "rf", data = trainingSet, 
                    trControl = trainControl(method = "cv", number = 5, allowParallel = TRUE))

# De-register parallel processing cluster

stopCluster(cluster)

modFitRFcv

plot(varImp(modFitRFcv), sub = "Random Forests Model Most Important Variables")
     
```


## Cross Validation & Expected Out of Sample Error

In order to calculate the expected out of sample error we perform a cross validation with the *validationSet*. This dataset was parked till now, so both models haven't seen it yet.

### Tree Model Out of Sample Error

```{r, Tree_based_cross_validation_chunk, echo = TRUE}

predTree <- predict(modFitTree, validationSet)

cmTree <- confusionMatrix(predTree, validationSet$classe)
cmTree$table
accuracyTree <- cmTree$overall[c(1, 3, 4)]
accuracyTree

outofsampleerrorTree <- 1 - accuracyTree[1]
outofsampleerrorTree[[1]]

```

The confusion matrix shows a siginificant number of misclassification errors where the *Prediction* doesn't match up with the *Reference*. An accuracy of `r round(100*accuracyTree[[1]], 1)`% or put differently an out of sample error rate of `r round(100*outofsampleerrorTree[[1]], 1)`% is pretty poor.


### Random Forests Model Out of Sample Error

```{r, Random_Forests_cross_validation_chunk, echo = TRUE}

predRF <- predict(modFitRFcv, validationSet)

cmRF <- confusionMatrix(predRF, validationSet$classe)
cmRF$table
accuracyRF <- cmRF$overall[c(1, 3, 4)]
accuracyRF

outofsampleerrorRF <- 1 - accuracyRF[1]
outofsampleerrorRF[[1]]

```

This time the confusion matrix only has a small number of misclassifiers. With an accuracy of `r round(100*accuracyRF[[1]], 1)`% and a respective out of sample error of `r round(100*outofsampleerrorRF[[1]], 1)`% we proceed with the random forests model as the chosen one for our prediction.


## Reasoning for Choices

Because the outcome *classe* is a categorical variable we started off with the *Tree Model* with its good interpretability. However, with such a poor *Tree Model* accuracy we moved on to the *Random Forests Model* to average the trees. This with the expectation to pump-up the accuracy. 

Initially, our first attempt was to use a default bootstrapping method within the *Random Forests Model*. This took way too much processor time and with a limit of 6GB RAM memory we switched to parallel processing combined with a 5-fold cross validation. This significanlty reduced the processing time to a couple of minutes while maintaining a sufficient high accuracy.

We also considered running a *Boosting Model*, yet due to *Random Forests Model* processing issues we refrained from doing so.

## Predicting Test Cases

We select the best model, which is the *Random Forests Model*. This final model is used to predict the *classe* outcome on the test cases.

```{r, predicting_test_cases_chunk, echo = TRUE}

predTest <- predict(modFitRFcv, testing)
predTest

```

## Conclusions

For predicting the quality of performing the Unilateral Dumbbell Biceps Curls we apply a *Random Forests Model* with a prediction accuracy of `r round(100*accuracyRF[[1]], 1)`%. This model is subsequently verified on 20 test cases with the goal of getting a quality label on the exercise. Ultimately this information can be used in a feedback loop towards the practitioner.


